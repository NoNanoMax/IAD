# Пробное задание ИАД
### Описание задачи
Обосновать и показать на примере почему повышать число слоев нейросети предпочтительнее, с точки зрения точности аппроксимации, чем повышать число нейронов скрытого слоя.

--------
### Теория
Можно ознакомиться в [презентации](https://github.com/NoNanoMax/IAD/blob/main/pres.pdf) 

--------
### Код и эксперименты
Создал простую программу, которая поддерживает:
1.  Два вида нейронных сетей, все слои - линейные + функция активации ReLU:
  > Однослойная - OneLayer
  > 
  > Многослойная - MultiLayer
2.  Загрузка датасетов:
  > Синтетические одномерные функции
  > 
  > Датасет MNIST

Чтобы протестировать работу программы выполнить следующий команды:

    git clone git@github.com:NoNanoMax/IAD.git
    
    python3 main.py --model1=OneLayer --layers1=25 model=MultiLayer --layers2=6,6,6,6 --save_gif=ex1.gif --func=func1
    
--------
### Результаты экспериментов для синтетических данных
  > Однослойная сеть - 9 нейронов, Многослойная - 2 слоя по 3 нейронов
![](https://github.com/NoNanoMax/IAD/blob/main/gifs/ex1.gif)

  > Однослойная сеть - 9 нейронов, Многослойная - 3 слоя по 3 нейронов
![](https://github.com/NoNanoMax/IAD/blob/main/gifs/ex2.gif)

--------
### Результаты на MNIST
  >График отобожает зависимость точности на отложенной выборке в зависимости от количества эпох обучения (на графике одно деление соответсвует 5 эпохам)
  >Первая модель - однослойная с 64 нейронами в скрытом слое, вторая модель - 2-слойная с 32 нейронами
![](https://github.com/NoNanoMax/IAD/blob/main/gifs/ex0.jpg)
